{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e15ba8f-e629-4f5b-947d-53a3e20c8f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, sys\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from Resnet20model import Resnet20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88923943-0591-4fda-a8e2-ae891f54adcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1929, -0.0139, -0.0794,  0.0647,  0.3185, -0.0417,  0.4083,  0.5119,\n",
      "         -0.0675, -0.3102]], grad_fn=<AddmmBackward0>)\n",
      "Total Number of Parameters: 273067\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# sanity check for the correctness of Resnet20 architecture\n",
    "dummy_input = torch.ones((1,3,32,32))\n",
    "dummy_instance = Resnet20()\n",
    "print(dummy_instance.forward(dummy_input))\n",
    "total_params = 1\n",
    "for p in dummy_instance.parameters():\n",
    "    temp = 1\n",
    "    for s in p.size():\n",
    "        temp *= s\n",
    "    total_params += temp\n",
    "    #print(p.size())\n",
    "print(f\"Total Number of Parameters: {total_params}\")\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb6432a0-ec77-4c97-a693-2dad50ed8865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10 as torchCIFAR10\n",
    "\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2023, 0.1994, 0.2010)\n",
    "rndimg_mean = (136.289, 129.273,122.668)\n",
    "rndimg_std  = (73.788, 73.672, 76.487)\n",
    "\n",
    "# specify preprocessing function\n",
    "transform_train = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std),\n",
    "                                     transforms.RandomCrop(size=32, padding=4),\n",
    "                                     transforms.RandomHorizontalFlip()])\n",
    "tensor_transform_train = transforms.Compose([transforms.Normalize(rndimg_mean, rndimg_std),\n",
    "                                     transforms.RandomCrop(size=32, padding=4),\n",
    "                                     transforms.RandomHorizontalFlip()])\n",
    "\n",
    "transform_val = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "\n",
    "transform_test = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "\n",
    "transform_visulaizer = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb0e9fe7-8519-4b2b-b122-75f513d08e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for tensor datasets. (Allows transforms)\n",
    "\n",
    "class OODTensorDataset:\n",
    "    def __init__(self, tensor_data, transforms=None):\n",
    "        self.tensors = tensor_data\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __getitem__(self, index):   \n",
    "        x = self.tensors[index]\n",
    "        if self.transforms:\n",
    "            x = self.transforms(x)\n",
    "        return x, -1                 # returing -1 for OOD labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.tensors.size()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75423733-28ca-4410-a5b6-09fdd1691433",
   "metadata": {},
   "source": [
    "##### Set up Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9787c69d-f7f7-4da9-aad6-7eb89a5f7967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data\\cifar10_trainval_F22.zip\n",
      "Extracting ./data\\cifar10_trainval_F22.zip to ./data\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ./data\\cifar10_trainval_F22.zip\n",
      "Extracting ./data\\cifar10_trainval_F22.zip to ./data\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ./data\\cifar10_test_F22.zip\n",
      "Extracting ./data\\cifar10_test_F22.zip to ./data\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from tools.dataset import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "DATA_ROOT = \"./data\"\n",
    "RANDOM_IMGS = \"./data/300K_random_images.npy\"\n",
    "TRAIN_BATCH_SIZE = 256\n",
    "VAL_BATCH_SIZE = 100\n",
    "OOD_BATCH_SIZE = 2\n",
    "\n",
    "# construct dataset\n",
    "train_set_in = CIFAR10(\n",
    "    root=DATA_ROOT, \n",
    "    mode='train', \n",
    "    download=True,\n",
    "    transform= transform_train\n",
    ")\n",
    "random_images_data = torch.permute(torch.from_numpy(np.load(RANDOM_IMGS)), (0,3,1,2)).float()  \n",
    "rand_img_set = OODTensorDataset(\n",
    "    random_images_data,\n",
    "    tensor_transform_train\n",
    ")\n",
    "val_set = CIFAR10(\n",
    "    root=DATA_ROOT, \n",
    "    mode='val', \n",
    "    download=True,\n",
    "    transform= transform_val\n",
    ")\n",
    "test_set = CIFAR10(\n",
    "    root=DATA_ROOT,\n",
    "    mode='test',\n",
    "    download=True,\n",
    "    transform=transform_test\n",
    ")\n",
    "torch_test_set = torchCIFAR10(\n",
    "    root=DATA_ROOT,\n",
    "    train=False,\n",
    "    transform=transform_test,\n",
    "    download=True\n",
    ")\n",
    "torch_vis_set = torchCIFAR10(\n",
    "    root=DATA_ROOT,\n",
    "    train=False,\n",
    "    transform=transform_visulaizer,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "\n",
    "# construct dataloader\n",
    "train_loader_in = DataLoader(\n",
    "    train_set_in, \n",
    "    batch_size= TRAIN_BATCH_SIZE,  \n",
    "    shuffle= True,   \n",
    "    num_workers=4\n",
    ")\n",
    "train_loader_ood = DataLoader(\n",
    "    rand_img_set,\n",
    "    batch_size = OOD_BATCH_SIZE,\n",
    "    shuffle = True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_set, \n",
    "    batch_size= VAL_BATCH_SIZE,  \n",
    "    shuffle= False,  \n",
    "    num_workers=4\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_set\n",
    ")\n",
    "torch_test_loader = DataLoader(\n",
    "    torch_test_set\n",
    ")\n",
    "torch_visualizer_loader = DataLoader(\n",
    "    torch_vis_set\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "281724ab-43a3-4553-9c2d-ea40de42795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_mean_and_std(npdata, N=1000):\n",
    "    pavg    = np.array([0.0, 0.0, 0.0])\n",
    "    pavg_sq = np.array([0.0, 0.0, 0.0])\n",
    "    for img in npdata[0:N]:\n",
    "        scale = 1/(img.shape[0]*img.shape[1])\n",
    "        pavg += np.sum(img*scale,(0,1))*(1/N)\n",
    "        pavg_sq += np.sum((img**2)*scale,(0,1))*(1/N)\n",
    "\n",
    "    std = np.sqrt(pavg_sq-(pavg**2))\n",
    "    print(pavg)\n",
    "    print(std)\n",
    "#estimate_mean_and_std(np.load(RANDOM_IMGS).astype(\"int32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8e3fdb6-6371-4c3d-83df-c1e36dd6e458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# specify the device for computation\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f879202-c465-4058-8da0-8447601cab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Resnet20().to(device)\n",
    "#net.load_state_dict(torch.load(\"saved_model/resnet20.pth\")[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a5dd508-6120-46b7-8c88-bbd475382d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "WARMUP_LR = .01\n",
    "MOMENTUM = 0.9\n",
    "REG = 5e-4\n",
    "optimizer = optim.SGD(net.parameters(), lr=WARMUP_LR, momentum=MOMENTUM, weight_decay=REG)\n",
    "\n",
    "EPOCHS = 150\n",
    "CHECKPOINT_FOLDER = \"./saved_model\"\n",
    "INITAL_LR = .1\n",
    "DECAY_EPOCHS = 60\n",
    "DECAY = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e167f0d2-42c2-4156-bb25-1b88295faedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/150\n",
      "avg training loss: 2.8940900718910263\n",
      "Validation loss: 1.7796, Validation accuracy: 0.3586\n",
      "Saving ...\n",
      "Epoch: 1/150\n",
      "avg training loss: 2.738651990890503\n",
      "Validation loss: 1.5484, Validation accuracy: 0.4634\n",
      "Saving ...\n",
      "Epoch: 2/150\n",
      "avg training loss: 2.5535314083099365\n",
      "Validation loss: 1.5653, Validation accuracy: 0.4942\n",
      "Saving ...\n",
      "Epoch: 3/150\n",
      "avg training loss: 2.4773104190826416\n",
      "Validation loss: 1.2715, Validation accuracy: 0.5618\n",
      "Saving ...\n",
      "Epoch: 4/150\n",
      "avg training loss: 2.38065242767334\n",
      "Validation loss: 1.2027, Validation accuracy: 0.5826\n",
      "Saving ...\n",
      "Epoch: 5/150\n",
      "avg training loss: 2.333941698074341\n",
      "Validation loss: 1.1613, Validation accuracy: 0.6106\n",
      "Saving ...\n",
      "Epoch: 6/150\n",
      "avg training loss: 2.30107045173645\n",
      "Validation loss: 1.0690, Validation accuracy: 0.6346\n",
      "Saving ...\n",
      "Epoch: 7/150\n",
      "avg training loss: 2.242527484893799\n",
      "Validation loss: 1.0273, Validation accuracy: 0.6630\n",
      "Saving ...\n",
      "Epoch: 8/150\n",
      "avg training loss: 2.228848934173584\n",
      "Validation loss: 1.0129, Validation accuracy: 0.6718\n",
      "Saving ...\n",
      "Epoch: 9/150\n",
      "avg training loss: 2.111964702606201\n",
      "Validation loss: 1.0312, Validation accuracy: 0.6612\n",
      "Epoch: 10/150\n",
      "avg training loss: 2.222320079803467\n",
      "Validation loss: 1.7862, Validation accuracy: 0.5554\n",
      "Epoch: 11/150\n",
      "avg training loss: 2.1897988319396973\n",
      "Validation loss: 1.0993, Validation accuracy: 0.6464\n",
      "Epoch: 12/150\n",
      "avg training loss: 2.162142753601074\n",
      "Validation loss: 0.9851, Validation accuracy: 0.6778\n",
      "Saving ...\n",
      "Epoch: 13/150\n",
      "avg training loss: 1.9877121448516846\n",
      "Validation loss: 0.8156, Validation accuracy: 0.7344\n",
      "Saving ...\n",
      "Epoch: 14/150\n",
      "avg training loss: 2.0727195739746094\n",
      "Validation loss: 0.8738, Validation accuracy: 0.7068\n",
      "Epoch: 15/150\n",
      "avg training loss: 2.065403938293457\n",
      "Validation loss: 0.8060, Validation accuracy: 0.7450\n",
      "Saving ...\n",
      "Epoch: 16/150\n",
      "avg training loss: 1.9081075191497803\n",
      "Validation loss: 0.7637, Validation accuracy: 0.7626\n",
      "Saving ...\n",
      "Epoch: 17/150\n",
      "avg training loss: 1.8857035636901855\n",
      "Validation loss: 0.7247, Validation accuracy: 0.7550\n",
      "Epoch: 18/150\n",
      "avg training loss: 1.9167836904525757\n",
      "Validation loss: 0.7327, Validation accuracy: 0.7664\n",
      "Saving ...\n",
      "Epoch: 19/150\n",
      "avg training loss: 2.0272717475891113\n",
      "Validation loss: 0.8852, Validation accuracy: 0.7314\n",
      "Epoch: 20/150\n",
      "avg training loss: 1.8714659214019775\n",
      "Validation loss: 0.6820, Validation accuracy: 0.7792\n",
      "Saving ...\n",
      "Epoch: 21/150\n",
      "avg training loss: 1.9949920177459717\n",
      "Validation loss: 0.9427, Validation accuracy: 0.7022\n",
      "Epoch: 22/150\n",
      "avg training loss: 1.8436331748962402\n",
      "Validation loss: 0.6314, Validation accuracy: 0.7900\n",
      "Saving ...\n",
      "Epoch: 23/150\n",
      "avg training loss: 1.8310437202453613\n",
      "Validation loss: 0.6673, Validation accuracy: 0.7814\n",
      "Epoch: 24/150\n",
      "avg training loss: 1.880000114440918\n",
      "Validation loss: 0.9354, Validation accuracy: 0.7126\n",
      "Epoch: 25/150\n",
      "avg training loss: 1.843791127204895\n",
      "Validation loss: 0.7075, Validation accuracy: 0.7724\n",
      "Epoch: 26/150\n",
      "avg training loss: 1.8629639148712158\n",
      "Validation loss: 0.6269, Validation accuracy: 0.7932\n",
      "Saving ...\n",
      "Epoch: 27/150\n",
      "avg training loss: 1.9929510354995728\n",
      "Validation loss: 0.6679, Validation accuracy: 0.7874\n",
      "Epoch: 28/150\n",
      "avg training loss: 1.799809217453003\n",
      "Validation loss: 0.6545, Validation accuracy: 0.7806\n",
      "Epoch: 29/150\n",
      "avg training loss: 1.808017611503601\n",
      "Validation loss: 0.6561, Validation accuracy: 0.7904\n",
      "Epoch: 30/150\n",
      "avg training loss: 1.881728172302246\n",
      "Validation loss: 0.6781, Validation accuracy: 0.7914\n",
      "Epoch: 31/150\n",
      "avg training loss: 1.9567373991012573\n",
      "Validation loss: 0.9161, Validation accuracy: 0.7068\n",
      "Epoch: 32/150\n",
      "avg training loss: 1.7411386966705322\n",
      "Validation loss: 0.6367, Validation accuracy: 0.8048\n",
      "Saving ...\n",
      "Epoch: 33/150\n",
      "avg training loss: 1.7225183248519897\n",
      "Validation loss: 0.6000, Validation accuracy: 0.8082\n",
      "Saving ...\n",
      "Epoch: 34/150\n",
      "avg training loss: 1.9330602884292603\n",
      "Validation loss: 0.7717, Validation accuracy: 0.7560\n",
      "Epoch: 35/150\n",
      "avg training loss: 1.7659361362457275\n",
      "Validation loss: 0.6318, Validation accuracy: 0.7926\n",
      "Epoch: 36/150\n",
      "avg training loss: 1.7176395654678345\n",
      "Validation loss: 0.5032, Validation accuracy: 0.8310\n",
      "Saving ...\n",
      "Epoch: 37/150\n",
      "avg training loss: 1.8487247228622437\n",
      "Validation loss: 0.7537, Validation accuracy: 0.7814\n",
      "Epoch: 38/150\n",
      "avg training loss: 1.673213243484497\n",
      "Validation loss: 0.5347, Validation accuracy: 0.8254\n",
      "Epoch: 39/150\n",
      "avg training loss: 1.7730755805969238\n",
      "Validation loss: 0.5822, Validation accuracy: 0.8124\n",
      "Epoch: 40/150\n",
      "avg training loss: 1.691645622253418\n",
      "Validation loss: 0.5565, Validation accuracy: 0.8184\n",
      "Epoch: 41/150\n",
      "avg training loss: 1.694725751876831\n",
      "Validation loss: 0.5453, Validation accuracy: 0.8208\n",
      "Epoch: 42/150\n",
      "avg training loss: 1.7151355743408203\n",
      "Validation loss: 0.5767, Validation accuracy: 0.8116\n",
      "Epoch: 43/150\n",
      "avg training loss: 1.759372353553772\n",
      "Validation loss: 0.5795, Validation accuracy: 0.8102\n",
      "Epoch: 44/150\n",
      "avg training loss: 1.6977146863937378\n",
      "Validation loss: 0.5216, Validation accuracy: 0.8246\n",
      "Epoch: 45/150\n",
      "avg training loss: 1.6643918752670288\n",
      "Validation loss: 0.5549, Validation accuracy: 0.8200\n",
      "Epoch: 46/150\n",
      "avg training loss: 1.774839162826538\n",
      "Validation loss: 0.6837, Validation accuracy: 0.7900\n",
      "Epoch: 47/150\n",
      "avg training loss: 1.7277125120162964\n",
      "Validation loss: 0.6082, Validation accuracy: 0.8028\n",
      "Epoch: 48/150\n",
      "avg training loss: 1.7960140705108643\n",
      "Validation loss: 0.6483, Validation accuracy: 0.7950\n",
      "Epoch: 49/150\n",
      "avg training loss: 1.6629648208618164\n",
      "Validation loss: 0.5261, Validation accuracy: 0.8280\n",
      "Epoch: 50/150\n",
      "avg training loss: 1.617846131324768\n",
      "Validation loss: 0.4763, Validation accuracy: 0.8444\n",
      "Saving ...\n",
      "Epoch: 51/150\n",
      "avg training loss: 1.8631086349487305\n",
      "Validation loss: 0.6478, Validation accuracy: 0.7960\n",
      "Epoch: 52/150\n",
      "avg training loss: 1.6580088138580322\n",
      "Validation loss: 0.4749, Validation accuracy: 0.8400\n",
      "Epoch: 53/150\n",
      "avg training loss: 1.7232859134674072\n",
      "Validation loss: 0.5872, Validation accuracy: 0.8062\n",
      "Epoch: 54/150\n",
      "avg training loss: 1.6976633071899414\n",
      "Validation loss: 0.5579, Validation accuracy: 0.8264\n",
      "Epoch: 55/150\n",
      "avg training loss: 1.6394084692001343\n",
      "Validation loss: 0.4820, Validation accuracy: 0.8382\n",
      "Epoch: 56/150\n",
      "avg training loss: 1.5910699367523193\n",
      "Validation loss: 0.4461, Validation accuracy: 0.8570\n",
      "Saving ...\n",
      "Epoch: 57/150\n",
      "avg training loss: 1.6991114616394043\n",
      "Validation loss: 0.7577, Validation accuracy: 0.7788\n",
      "Epoch: 58/150\n",
      "avg training loss: 1.7262731790542603\n",
      "Validation loss: 0.6651, Validation accuracy: 0.7912\n",
      "Epoch: 59/150\n",
      "avg training loss: 1.6223211288452148\n",
      "Validation loss: 0.4860, Validation accuracy: 0.8438\n",
      "Epoch: 60/150\n",
      "avg training loss: 1.5758004188537598\n",
      "Validation loss: 0.4456, Validation accuracy: 0.8556\n",
      "Epoch: 61/150\n",
      "avg training loss: 1.7558209896087646\n",
      "Validation loss: 0.6424, Validation accuracy: 0.8028\n",
      "Epoch: 62/150\n",
      "avg training loss: 1.6563854217529297\n",
      "Validation loss: 0.5221, Validation accuracy: 0.8354\n",
      "Epoch: 63/150\n",
      "avg training loss: 1.629894733428955\n",
      "Validation loss: 0.4598, Validation accuracy: 0.8502\n",
      "Epoch: 64/150\n",
      "avg training loss: 1.6533156633377075\n",
      "Validation loss: 0.5346, Validation accuracy: 0.8316\n",
      "Epoch: 65/150\n",
      "avg training loss: 1.5466593503952026\n",
      "Validation loss: 0.4345, Validation accuracy: 0.8594\n",
      "Saving ...\n",
      "Epoch: 66/150\n",
      "avg training loss: 1.7635467052459717\n",
      "Validation loss: 0.8245, Validation accuracy: 0.7550\n",
      "Epoch: 67/150\n",
      "avg training loss: 1.6010451316833496\n",
      "Validation loss: 0.4415, Validation accuracy: 0.8580\n",
      "Epoch: 68/150\n",
      "avg training loss: 1.6071476936340332\n",
      "Validation loss: 0.4792, Validation accuracy: 0.8468\n",
      "Epoch: 69/150\n",
      "avg training loss: 1.5361844301223755\n",
      "Validation loss: 0.4768, Validation accuracy: 0.8436\n",
      "Epoch: 70/150\n",
      "avg training loss: 1.633504867553711\n",
      "Validation loss: 0.4662, Validation accuracy: 0.8496\n",
      "Epoch: 71/150\n",
      "avg training loss: 1.6035270690917969\n",
      "Validation loss: 0.6048, Validation accuracy: 0.8134\n",
      "Epoch: 72/150\n",
      "avg training loss: 1.576573133468628\n",
      "Validation loss: 0.4538, Validation accuracy: 0.8566\n",
      "Epoch: 73/150\n",
      "avg training loss: 1.6169030666351318\n",
      "Validation loss: 0.5362, Validation accuracy: 0.8310\n",
      "Epoch: 74/150\n",
      "avg training loss: 1.5968756675720215\n",
      "Validation loss: 0.4863, Validation accuracy: 0.8466\n",
      "Epoch: 75/150\n",
      "avg training loss: 1.5439776182174683\n",
      "Validation loss: 0.4144, Validation accuracy: 0.8688\n",
      "Saving ...\n",
      "Epoch: 76/150\n",
      "avg training loss: 1.5631645917892456\n",
      "Validation loss: 0.4656, Validation accuracy: 0.8424\n",
      "Epoch: 77/150\n",
      "avg training loss: 1.7898519039154053\n",
      "Validation loss: 0.7118, Validation accuracy: 0.7788\n",
      "Epoch: 78/150\n",
      "avg training loss: 1.5672069787979126\n",
      "Validation loss: 0.4426, Validation accuracy: 0.8544\n",
      "Epoch: 79/150\n",
      "avg training loss: 1.649806261062622\n",
      "Validation loss: 0.7100, Validation accuracy: 0.7832\n",
      "Epoch: 80/150\n",
      "avg training loss: 1.5230777263641357\n",
      "Validation loss: 0.3990, Validation accuracy: 0.8700\n",
      "Saving ...\n",
      "Epoch: 81/150\n",
      "avg training loss: 1.6535650491714478\n",
      "Validation loss: 0.5356, Validation accuracy: 0.8352\n",
      "Epoch: 82/150\n",
      "avg training loss: 1.5700671672821045\n",
      "Validation loss: 0.4471, Validation accuracy: 0.8552\n",
      "Epoch: 83/150\n",
      "avg training loss: 1.551478624343872\n",
      "Validation loss: 0.4424, Validation accuracy: 0.8614\n",
      "Epoch: 84/150\n",
      "avg training loss: 1.5523780584335327\n",
      "Validation loss: 0.4259, Validation accuracy: 0.8586\n",
      "Epoch: 85/150\n",
      "avg training loss: 1.5574445724487305\n",
      "Validation loss: 0.4407, Validation accuracy: 0.8600\n",
      "Epoch: 86/150\n",
      "avg training loss: 1.806579351425171\n",
      "Validation loss: 0.5518, Validation accuracy: 0.8306\n",
      "Epoch: 87/150\n",
      "avg training loss: 1.5816571712493896\n",
      "Validation loss: 0.4899, Validation accuracy: 0.8422\n",
      "Epoch: 88/150\n",
      "avg training loss: 1.5545309782028198\n",
      "Validation loss: 0.4396, Validation accuracy: 0.8584\n",
      "Epoch: 89/150\n",
      "avg training loss: 1.6709296703338623\n",
      "Validation loss: 0.5083, Validation accuracy: 0.8358\n",
      "Epoch: 90/150\n",
      "avg training loss: 1.7068777084350586\n",
      "Validation loss: 0.7703, Validation accuracy: 0.7608\n",
      "Epoch: 91/150\n",
      "avg training loss: 1.6255919933319092\n",
      "Validation loss: 0.4847, Validation accuracy: 0.8540\n",
      "Epoch: 92/150\n",
      "avg training loss: 1.540440559387207\n",
      "Validation loss: 0.4171, Validation accuracy: 0.8668\n",
      "Epoch: 93/150\n",
      "avg training loss: 1.6808425188064575\n",
      "Validation loss: 0.5944, Validation accuracy: 0.8072\n",
      "Epoch: 94/150\n",
      "avg training loss: 1.5026795864105225\n",
      "Validation loss: 0.4069, Validation accuracy: 0.8690\n",
      "Epoch: 95/150\n",
      "avg training loss: 1.5098419189453125\n",
      "Validation loss: 0.4335, Validation accuracy: 0.8620\n",
      "Epoch: 96/150\n",
      "avg training loss: 1.5784859657287598\n",
      "Validation loss: 0.4066, Validation accuracy: 0.8670\n",
      "Epoch: 97/150\n",
      "avg training loss: 1.594369649887085\n",
      "Validation loss: 0.4735, Validation accuracy: 0.8422\n",
      "Epoch: 98/150\n",
      "avg training loss: 1.6133331060409546\n",
      "Validation loss: 0.5128, Validation accuracy: 0.8376\n",
      "Epoch: 99/150\n",
      "avg training loss: 1.6299629211425781\n",
      "Validation loss: 0.5501, Validation accuracy: 0.8348\n",
      "Epoch: 100/150\n",
      "avg training loss: 1.5890631675720215\n",
      "Validation loss: 0.4986, Validation accuracy: 0.8442\n",
      "Epoch: 101/150\n",
      "avg training loss: 1.5346145629882812\n",
      "Validation loss: 0.4128, Validation accuracy: 0.8668\n",
      "Epoch: 102/150\n",
      "avg training loss: 1.6892731189727783\n",
      "Validation loss: 0.6025, Validation accuracy: 0.8102\n",
      "Epoch: 103/150\n",
      "avg training loss: 1.6652764081954956\n",
      "Validation loss: 0.4987, Validation accuracy: 0.8402\n",
      "Epoch: 104/150\n",
      "avg training loss: 1.5930391550064087\n",
      "Validation loss: 0.4813, Validation accuracy: 0.8482\n",
      "Epoch: 105/150\n",
      "avg training loss: 1.5561158657073975\n",
      "Validation loss: 0.4850, Validation accuracy: 0.8436\n",
      "Epoch: 106/150\n",
      "avg training loss: 1.6029958724975586\n",
      "Validation loss: 0.4717, Validation accuracy: 0.8430\n",
      "Epoch: 107/150\n",
      "avg training loss: 1.5842950344085693\n",
      "Validation loss: 0.4592, Validation accuracy: 0.8534\n",
      "Epoch: 108/150\n",
      "avg training loss: 1.4849027395248413\n",
      "Validation loss: 0.4067, Validation accuracy: 0.8664\n",
      "Epoch: 109/150\n",
      "avg training loss: 1.4914950132369995\n",
      "Validation loss: 0.3954, Validation accuracy: 0.8748\n",
      "Saving ...\n",
      "Epoch: 110/150\n",
      "avg training loss: 1.5925556421279907\n",
      "Validation loss: 0.4787, Validation accuracy: 0.8506\n",
      "Epoch: 111/150\n",
      "avg training loss: 1.5013214349746704\n",
      "Validation loss: 0.4241, Validation accuracy: 0.8682\n",
      "Epoch: 112/150\n",
      "avg training loss: 1.6021255254745483\n",
      "Validation loss: 0.5057, Validation accuracy: 0.8426\n",
      "Epoch: 113/150\n",
      "avg training loss: 1.4393624067306519\n",
      "Validation loss: 0.3725, Validation accuracy: 0.8732\n",
      "Epoch: 114/150\n",
      "avg training loss: 1.587501049041748\n",
      "Validation loss: 0.5283, Validation accuracy: 0.8358\n",
      "Epoch: 115/150\n",
      "avg training loss: 1.7632266283035278\n",
      "Validation loss: 0.5078, Validation accuracy: 0.8394\n",
      "Epoch: 116/150\n",
      "avg training loss: 1.6764860153198242\n",
      "Validation loss: 0.6328, Validation accuracy: 0.7992\n",
      "Epoch: 117/150\n",
      "avg training loss: 1.5230462551116943\n",
      "Validation loss: 0.4587, Validation accuracy: 0.8484\n",
      "Epoch: 118/150\n",
      "avg training loss: 1.4938886165618896\n",
      "Validation loss: 0.3998, Validation accuracy: 0.8690\n",
      "Epoch: 119/150\n",
      "avg training loss: 1.5843778848648071\n",
      "Validation loss: 0.5734, Validation accuracy: 0.8250\n",
      "Epoch: 120/150\n",
      "avg training loss: 1.4979932308197021\n",
      "Validation loss: 0.3862, Validation accuracy: 0.8740\n",
      "Epoch: 121/150\n",
      "avg training loss: 1.4360955953598022\n",
      "Validation loss: 0.3695, Validation accuracy: 0.8822\n",
      "Saving ...\n",
      "Epoch: 122/150\n",
      "avg training loss: 1.447453498840332\n",
      "Validation loss: 0.3558, Validation accuracy: 0.8866\n",
      "Saving ...\n",
      "Epoch: 123/150\n",
      "avg training loss: 1.4120289087295532\n",
      "Validation loss: 0.3553, Validation accuracy: 0.8840\n",
      "Epoch: 124/150\n",
      "avg training loss: 1.4112244844436646\n",
      "Validation loss: 0.3429, Validation accuracy: 0.8872\n",
      "Saving ...\n",
      "Epoch: 125/150\n",
      "avg training loss: 1.4356454610824585\n",
      "Validation loss: 0.3484, Validation accuracy: 0.8890\n",
      "Saving ...\n",
      "Epoch: 126/150\n",
      "avg training loss: 1.4170793294906616\n",
      "Validation loss: 0.3373, Validation accuracy: 0.8896\n",
      "Saving ...\n",
      "Epoch: 127/150\n",
      "avg training loss: 1.3836963176727295\n",
      "Validation loss: 0.3363, Validation accuracy: 0.8906\n",
      "Saving ...\n",
      "Epoch: 128/150\n",
      "avg training loss: 1.3688832521438599\n",
      "Validation loss: 0.3316, Validation accuracy: 0.8916\n",
      "Saving ...\n",
      "Epoch: 129/150\n",
      "avg training loss: 1.4741113185882568\n",
      "Validation loss: 0.3474, Validation accuracy: 0.8886\n",
      "Epoch: 130/150\n",
      "avg training loss: 1.3777172565460205\n",
      "Validation loss: 0.3361, Validation accuracy: 0.8920\n",
      "Saving ...\n",
      "Epoch: 131/150\n",
      "avg training loss: 1.4316010475158691\n",
      "Validation loss: 0.3434, Validation accuracy: 0.8884\n",
      "Epoch: 132/150\n",
      "avg training loss: 1.3978710174560547\n",
      "Validation loss: 0.3379, Validation accuracy: 0.8938\n",
      "Saving ...\n",
      "Epoch: 133/150\n",
      "avg training loss: 1.410407543182373\n",
      "Validation loss: 0.3291, Validation accuracy: 0.8918\n",
      "Epoch: 134/150\n",
      "avg training loss: 1.3650975227355957\n",
      "Validation loss: 0.3295, Validation accuracy: 0.8944\n",
      "Saving ...\n",
      "Epoch: 135/150\n",
      "avg training loss: 1.3967437744140625\n",
      "Validation loss: 0.3252, Validation accuracy: 0.8944\n",
      "Epoch: 136/150\n",
      "avg training loss: 1.373824954032898\n",
      "Validation loss: 0.3226, Validation accuracy: 0.8976\n",
      "Saving ...\n",
      "Epoch: 137/150\n",
      "avg training loss: 1.3634777069091797\n",
      "Validation loss: 0.3203, Validation accuracy: 0.8968\n",
      "Epoch: 138/150\n",
      "avg training loss: 1.363832712173462\n",
      "Validation loss: 0.3226, Validation accuracy: 0.8970\n",
      "Epoch: 139/150\n",
      "avg training loss: 1.4045915603637695\n",
      "Validation loss: 0.3257, Validation accuracy: 0.8946\n",
      "Epoch: 140/150\n",
      "avg training loss: 1.4046950340270996\n",
      "Validation loss: 0.3266, Validation accuracy: 0.8950\n",
      "Epoch: 141/150\n",
      "avg training loss: 1.3680400848388672\n",
      "Validation loss: 0.3231, Validation accuracy: 0.8976\n",
      "Epoch: 142/150\n",
      "avg training loss: 1.345055103302002\n",
      "Validation loss: 0.3254, Validation accuracy: 0.8946\n",
      "Epoch: 143/150\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    avg_loss = 0\n",
    "    best_val_acc = 0\n",
    "    current_learning_rate = WARMUP_LR\n",
    "    train_loader_ood.dataset.offset = np.random.randint(len(train_loader_ood.dataset))\n",
    "    #Learning rate scheduler\n",
    "    for i in range(EPOCHS):\n",
    "        if i == 5:\n",
    "            current_learning_rate = INITAL_LR\n",
    "        if i % DECAY_EPOCHS == 0 and i != 0:\n",
    "            current_learning_rate = current_learning_rate * DECAY\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = current_learning_rate\n",
    "            #print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
    "    \n",
    "        net.train()\n",
    "        \n",
    "        print(f\"Epoch: {i}/{EPOCHS}\")\n",
    "        for in_set, out_set in zip(train_loader_in, train_loader_ood):\n",
    "            \n",
    "            data = torch.cat((in_set[0], out_set[0]), 0)\n",
    "            target = in_set[1].type(torch.LongTensor)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Forward Pass\n",
    "            x = net.forward(data)\n",
    "            \n",
    "            loss = F.cross_entropy(x[:len(in_set[0])], target)\n",
    "            # cross-entropy from softmax distribution to uniform distribution\n",
    "            loss += 0.5 * -(x[len(in_set[0]):].mean(1) - torch.logsumexp(x[len(in_set[0]):], dim=1)).mean()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_loss = .8 * avg_loss + float(loss) * 0.2\n",
    "       \n",
    "        print(f\"avg training loss: {avg_loss}\")\n",
    "        \n",
    "        \n",
    "        net.eval()\n",
    "\n",
    "        total_examples = 0\n",
    "        correct_examples = 0\n",
    "\n",
    "        val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "                # copy inputs to device\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.type(torch.LongTensor)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # compute the output and loss\n",
    "                output = net.forward(inputs)\n",
    "                loss = F.cross_entropy(output, targets)\n",
    "                val_loss += loss\n",
    "\n",
    "                # count the number of correctly predicted samples in the current batch\n",
    "                total_examples += targets.size()[0]\n",
    "                batch_preds = torch.argmax(output, dim=1)\n",
    "                correct_examples += (batch_preds == targets).int().sum().item()\n",
    "\n",
    "        avg_loss = val_loss / len(val_loader)\n",
    "        avg_acc = correct_examples / total_examples\n",
    "        print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
    "        \n",
    "        #save the model checkpoint\n",
    "        if avg_acc > best_val_acc:\n",
    "            best_val_acc = avg_acc\n",
    "            if not os.path.exists(CHECKPOINT_FOLDER):\n",
    "               os.makedirs(CHECKPOINT_FOLDER)\n",
    "            print(\"Saving ...\")\n",
    "            state = {'state_dict': cnn.state_dict(),\n",
    "                    'epoch': i,\n",
    "                    'lr': current_learning_rate}\n",
    "            torch.save(state, os.path.join(CHECKPOINT_FOLDER, 'OE_resnet20.pth'))\n",
    "\n",
    "    \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ad35a5-5a92-4a6e-ab20-46b199f2bf9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
